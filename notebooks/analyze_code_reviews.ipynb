{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13d7e989",
   "metadata": {},
   "source": [
    "# Code Review Analyzer Demo\n",
    "\n",
    "This notebook demonstrates how to use the Code Review Analyzer to analyze and evaluate code review comments from different AI code review bots.\n",
    "\n",
    "## Setup\n",
    "First, let's install the required packages and configure our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f70bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install aiohttp google-generativeai anthropic openai pandas matplotlib seaborn python-dotenv pydantic typing-extensions\n",
    "\n",
    "# Clone the repository (if running in Colab)\n",
    "!git clone https://github.com/yourusername/CodeReviewAnalyzer.git\n",
    "!cd CodeReviewAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48520a48",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Enter your API keys below. These will be stored only for this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69bfc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load from .env file if it exists\n",
    "load_dotenv()\n",
    "\n",
    "# Set your API keys here (or in .env file)\n",
    "os.environ['GITHUB_TOKEN'] = os.getenv('GITHUB_TOKEN', 'your_github_token')\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY', 'your_gemini_api_key')\n",
    "os.environ['ANTHROPIC_KEY'] = os.getenv('ANTHROPIC_KEY', 'your_claude_api_key') \n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your_openai_api_key')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d39269",
   "metadata": {},
   "source": [
    "## Importing Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897ddbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "from code_review_analyzer.github import GitHubAPI\n",
    "from code_review_analyzer.analyzers import GeminiReviewAnalyzer, ClaudeReviewAnalyzer, GPT4ReviewAnalyzer\n",
    "from code_review_analyzer.visualization import ResultsVisualizer\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9b6db1",
   "metadata": {},
   "source": [
    "## Analyzing Code Reviews\n",
    "Now let's analyze some code reviews from a GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba0aa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def analyze_reviews(repo_name: str, num_prs: int = 5):\n",
    "    \"\"\"\n",
    "    Analyze code reviews from a GitHub repository\n",
    "    \n",
    "    Args:\n",
    "        repo_name: GitHub repository in format 'owner/repo'\n",
    "        num_prs: Number of most recent PRs to analyze\n",
    "    \"\"\"\n",
    "    # Initialize components\n",
    "    github = GitHubAPI(os.environ['GITHUB_TOKEN'], repo_name)\n",
    "    \n",
    "    # Initialize analyzers\n",
    "    analyzers = {\n",
    "        'gemini': GeminiReviewAnalyzer(os.environ['GOOGLE_API_KEY']),\n",
    "        'claude': ClaudeReviewAnalyzer(os.environ['ANTHROPIC_KEY']),\n",
    "        'gpt4': GPT4ReviewAnalyzer(os.environ['OPENAI_API_KEY'])\n",
    "    }\n",
    "    \n",
    "    visualizer = ResultsVisualizer()\n",
    "    \n",
    "    try:\n",
    "        # Fetch recent PRs\n",
    "        logger.info(f\"Fetching {num_prs} recent PRs from {repo_name}...\")\n",
    "        prs = await github.fetch_recent_prs(limit=num_prs)\n",
    "        \n",
    "        # Collect and analyze comments\n",
    "        all_comments = []\n",
    "        for pr in prs:\n",
    "            pr_number = pr['number']\n",
    "            logger.info(f\"Processing PR #{pr_number}...\")\n",
    "            \n",
    "            # Fetch PR data\n",
    "            comments = await github.fetch_pr_comments(pr_number)\n",
    "            diff = await github.fetch_pr_diff(pr_number)\n",
    "            \n",
    "            # Analyze with each bot\n",
    "            for name, analyzer in analyzers.items():\n",
    "                bot_comments = await analyzer.analyze_diff(diff)\n",
    "                all_comments.extend(bot_comments)\n",
    "                \n",
    "            # Add existing bot comments\n",
    "            all_comments.extend(comments)\n",
    "        \n",
    "        # Analyze comment quality\n",
    "        logger.info(\"Analyzing comment quality...\")\n",
    "        metrics = {}\n",
    "        for name, analyzer in analyzers.items():\n",
    "            metrics[name] = await analyzer.analyze_comment_quality(all_comments)\n",
    "        \n",
    "        # Generate visualizations\n",
    "        logger.info(\"Generating visualizations...\")\n",
    "        output_dir = 'analysis_results'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        visualizer.create_impact_distribution_chart(\n",
    "            metrics,\n",
    "            os.path.join(output_dir, 'comment_distribution.png')\n",
    "        )\n",
    "        \n",
    "        visualizer.create_bot_comparison_chart(\n",
    "            metrics,\n",
    "            os.path.join(output_dir, 'bot_comparison.png')\n",
    "        )\n",
    "        \n",
    "        visualizer.save_metrics_report(\n",
    "            metrics,\n",
    "            os.path.join(output_dir, 'analysis_report.txt')\n",
    "        )\n",
    "        \n",
    "        visualizer.export_to_excel(\n",
    "            metrics,\n",
    "            os.path.join(output_dir, 'metrics.xlsx')\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"\"\"\n",
    "        Analysis complete! Results saved in {output_dir}:\n",
    "        1. comment_distribution.png - Visual breakdown of comment categories\n",
    "        2. bot_comparison.png - Radar chart comparing bot performance\n",
    "        3. analysis_report.txt - Detailed metrics and statistics\n",
    "        4. metrics.xlsx - Excel file with all metrics\n",
    "        \"\"\")\n",
    "        \n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during analysis: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf7edc5",
   "metadata": {},
   "source": [
    "## Running the Analysis\n",
    "Let's analyze a repository. You can modify the repository name and number of PRs to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1609135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repository to analyze\n",
    "REPO_NAME = \"microsoft/typescript\"  # Example repository\n",
    "NUM_PRS = 5  # Number of PRs to analyze\n",
    "\n",
    "# Run the analysis\n",
    "metrics = await analyze_reviews(REPO_NAME, NUM_PRS)\n",
    "\n",
    "# Display results\n",
    "from IPython.display import Image\n",
    "Image('analysis_results/comment_distribution.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8095cb",
   "metadata": {},
   "source": [
    "## Viewing the Results\n",
    "The analysis results are saved in the `analysis_results` directory. You can also explore the metrics dictionary directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5e7b97",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "# Print metrics summary\n",
    "for bot, data in metrics.items():\n",
    "    print(f\"\\nBot: {bot}\")\n",
    "    print(\"-\" * 20)\n",
    "    for metric, value in data.items():\n",
    "        if metric == 'total_comments':\n",
    "            print(f\"{metric}: {value}\")\n",
    "        else:\n",
    "            print(f\"{metric}: {value:.1%}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
